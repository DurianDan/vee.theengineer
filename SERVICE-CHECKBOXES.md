### Creating a Centralized Data Catalog for Scattered Data Sources

**Questions:**

- [ ] What data sources need to be centralized (e.g., APIs, databases, files)?
- [ ] How many data sources are involved, and what are their formats?
- [ ] Is there an existing data storage system you want to integrate, or should a new one be set up?
- [ ] Do you have a preferred data cataloging tool or framework?
- [ ] What kind of metadata should be included in the catalog (e.g., data descriptions, lineage, owners)?
- [ ] Will access controls or permissions be required for the catalog?
- [ ] Are there specific requirements for querying or searching the catalog?
- [ ] Should the catalog support real-time updates, or will it be static?
- [ ] What are the desired hosting options (cloud-based, on-premises, hybrid)?

---
### Web Scraping for Market Trends or Public Information

**Questions:**

- [ ] What specific data or public information do you need scraped?
- [ ] From which websites or platforms should the data be collected?
- [ ] Are there any restrictions or ethical considerations for scraping these sources?
- [ ] In what format do you need the scraped data (CSV, JSON, database, etc.)?
- [ ] What is the desired frequency of scraping (one-time, daily, weekly, etc.)?
- [ ] Are there any data cleaning or processing requirements for the scraped data?
- [ ] Is there a budget or limit for accessing paid APIs, proxies, or tools?
- [ ] Are there any specific technologies or libraries you prefer for the scraping process?
- [ ] Will the project require bypassing CAPTCHAs or other anti-bot measures?

---
### Building ETL/ELT Pipelines for Analytics

**Questions:**

- [ ] What are the source systems and formats for the data to be extracted?
- [ ] Where will the transformed data be loaded (data warehouse, lake, dashboard, etc.)?
- [ ] What specific transformations or cleaning steps are required?
- [ ] Do you need real-time data pipelines or batch processing?
- [ ] Are there volume or scalability considerations for the pipelines?
- [ ] Should the pipelines be built using specific tools (e.g., Apache Airflow, Talend, dbt)?
- [ ] What are the latency and performance requirements for data delivery?
- [ ] Are there monitoring, alerting, or logging requirements for the pipelines?
- [ ] Do you have any existing workflows or systems to integrate with?

By collecting these details systematically, you can clearly define the scope and identify challenges for each service.